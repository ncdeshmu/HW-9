[
  {
    "objectID": "HW-9.html",
    "href": "HW-9.html",
    "title": "HW-9",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.2\n\n\nWarning: package 'ggplot2' was built under R version 4.4.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n\n\nWarning: package 'dials' was built under R version 4.4.2\n\n\nWarning: package 'infer' was built under R version 4.4.2\n\n\nWarning: package 'modeldata' was built under R version 4.4.2\n\n\nWarning: package 'parsnip' was built under R version 4.4.2\n\n\nWarning: package 'recipes' was built under R version 4.4.2\n\n\nWarning: package 'rsample' was built under R version 4.4.2\n\n\nWarning: package 'tune' was built under R version 4.4.2\n\n\nWarning: package 'workflows' was built under R version 4.4.2\n\n\nWarning: package 'workflowsets' was built under R version 4.4.2\n\n\nWarning: package 'yardstick' was built under R version 4.4.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nset.seed(10)\nbike_data &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/bikeDetails.csv\")\n\nRows: 1061 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): name, seller_type, owner\ndbl (4): selling_price, year, km_driven, ex_showroom_price\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nbike_data &lt;- bike_data |&gt; \n  mutate(log_selling_price = log(selling_price), \n         log_km_driven = log(km_driven),\n         owners = ifelse(owner == \"1st owner\", \"single\", \"multiple\")) |&gt;\n  select(log_km_driven, log_selling_price, everything())\n#use tidymodel functions for splitting the data\nbike_split &lt;- initial_split(bike_data, prop = 0.7)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\n\n\n#create folds\nbike_CV_folds &lt;- vfold_cv(bike_train, 10)\n\n\nlm(log_selling_price ~ log_km_driven + year + owners, data = bike_train)$coef\n\n  (Intercept) log_km_driven          year  ownerssingle \n-134.57077580   -0.24380816    0.07339603   -0.08397016 \n\n\nLASSO Model\n\nLASSO_recipe &lt;- recipe(log_selling_price ~ log_km_driven + owners + year, \n                      data = bike_train) |&gt;\n  step_dummy(owners) |&gt;\n  step_normalize(log_km_driven, year)\n\n\nLASSO_spec &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\n\nLASSO_wkf &lt;- workflow() |&gt;\n  add_recipe(LASSO_recipe) |&gt;\n  add_model(LASSO_spec)\nLASSO_wkf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\n#LASSO_grid &lt;- LASSO_wkf |&gt;\n  #tune_grid(resamples = bike_CV_folds,\n            #grid = grid_regular(penalty(), levels = 100)) \n\nRegression Tree Model\n\nlibrary(tree) #rpart is also often used\n\nWarning: package 'tree' was built under R version 4.4.2\n\nfitTree &lt;- tree(dist ~ speed, data = cars) #default splitting is deviance\nplot(fitTree)\ntext(fitTree)\n\n\n\n\n\n\n\n\n\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\nBagged Tree\n\nbike_data &lt;- bike_data |&gt; \n  mutate(log_selling_price = log(selling_price), \n         log_km_driven = log(km_driven),\n         owners = ifelse(owner == \"1st owner\", \"single\", \"multiple\")) |&gt;\n  select(log_km_driven, log_selling_price, everything())\n#use tidymodel functions for splitting the data\nbike_split &lt;- initial_split(bike_data, prop = 0.7)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\n\n\nLASSO_recipe &lt;- recipe(log_selling_price ~ log_km_driven + owners + year, \n                      data = bike_train)|&gt;\n  step_dummy(owners) |&gt;\n  step_normalize(log_km_driven, year)\n#LR3_rec |&gt;\n #prep(bike_train) |&gt;\n #bake(bike_train) |&gt;\n #colnames()\n\n\nbag_spec &lt;- bag_tree(tree_depth = 5, min_n = 10, cost_complexity = tune()) |&gt;\n set_engine(\"rpart\") |&gt;\n set_mode(\"classification\")\n\n\n#library(baguette)\n#bag_wkf &lt;- workflow() |&gt;\n #add_recipe(#LR3_rec) |&gt;\n #add_model(bag_spec)\n\n\n#bag_fit &lt;- bag_wkf |&gt;\n #tune_grid(resamples = heart_CV_folds,\n #grid = grid_regular(cost_complexity(),\n #levels = 15),\n #metrics = metric_set(accuracy, mn_log_loss))\n#bag_fit\n\n\n#bag_fit |&gt;\n #collect_metrics() |&gt;\n #filter(.metric == \"mn_log_loss\") |&gt;\n #arrange(mean)\n\nRandom Forest\n\nrf_spec &lt;- rand_forest(mtry = tune()) |&gt;\n set_engine(\"ranger\") |&gt;\n set_mode(\"classification\")\n\n\n#rf_wkf &lt;- workflow() |&gt;\n #add_recipe(LR3_rec) |&gt;\n #add_model(rf_spec)\n\n\n#rf_fit &lt;- rf_wkf |&gt;\n #tune_grid(resamples = bike_CV_folds,\n #grid = 7,\n #metrics = metric_set(accuracy, mn_log_loss))\n\nPulling Out the Best Model\n\n#lowest_rmse &lt;- LASSO_grid |&gt;\n  #select_best(metric = \"rmse\")\n#lowest_rmse\n\n\n#rbind(MLR_fit1 |&gt; collect_metrics() |&gt; filter(.metric == \"rmse\"),\n      #MLR_fit2 |&gt; collect_metrics() |&gt; filter(.metric == \"rmse\"),\n      #MLR_fit3 |&gt; collect_metrics() |&gt; filter(.metric == \"rmse\")) |&gt; \n  #mutate(Model = c(\"Model 1\", \"Model 2\", \"Model 3\")) |&gt;\n  #select(Model, mean, n, std_err)"
  }
]